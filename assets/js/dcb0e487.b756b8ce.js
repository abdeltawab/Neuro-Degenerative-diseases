"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9350],{9710:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>d});var t=i(4848),o=i(8453);const a={title:"VAME step-by-step",sidebar_position:2},r=void 0,s={id:"getting_started/step_by_step",title:"VAME step-by-step",description:"The VAME workflow consists of four main steps, plus optional steps to analyse your data:",source:"@site/docs/getting_started/step_by_step.mdx",sourceDirName:"getting_started",slug:"/getting_started/step_by_step",permalink:"/VAME/docs/getting_started/step_by_step",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"VAME step-by-step",sidebar_position:2},sidebar:"docsSidebar",previous:{title:"Installation",permalink:"/VAME/docs/getting_started/installation"},next:{title:"Run Pipeline",permalink:"/VAME/docs/getting_started/pipeline"}},l={},d=[{value:"0. [Optional] Download input data",id:"0-optional-download-input-data",level:2},{value:"1. Initialize the project",id:"1-initialize-the-project",level:2},{value:"2. Preprocess the data",id:"2-preprocess-the-data",level:2},{value:"2.1 Clean low confidence data points",id:"21-clean-low-confidence-data-points",level:3},{value:"2.2 Egocentric alignment",id:"22-egocentric-alignment",level:3},{value:"2.3 Clean outliers",id:"23-clean-outliers",level:3},{value:"2.4 Savitzky-Golay filter",id:"24-savitzky-golay-filter",level:3},{value:"3. Train the neural network",id:"3-train-the-neural-network",level:2},{value:"3.1 Create the training dataset",id:"31-create-the-training-dataset",level:3},{value:"3.2 Training the model",id:"32-training-the-model",level:3},{value:"3.3 Evaluate the model",id:"33-evaluate-the-model",level:3},{value:"4. Segment behavior",id:"4-segment-behavior",level:2},{value:"4.1 Pose segmentation",id:"41-pose-segmentation",level:3},{value:"4.2 Community detection",id:"42-community-detection",level:3},{value:"5. Visualize and analyze",id:"5-visualize-and-analyze",level:2},{value:"5.1 Creating motif and community videos",id:"51-creating-motif-and-community-videos",level:3},{value:"5.2 UMAP visualization",id:"52-umap-visualization",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"The VAME workflow consists of four main steps, plus optional steps to analyse your data:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Initialize project: This is step is responsible by starting the project, getting your pose estimation data into the right format"}),"\n",(0,t.jsx)(n.li,{children:"Preprocess: This step is responsible for cleaning, filtering and aligning the pose estimation data"}),"\n",(0,t.jsxs)(n.li,{children:["Train neural network:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Create a training dataset for the VAME deep learning model."}),"\n",(0,t.jsx)(n.li,{children:"Train a variational autoencoder which is parameterized with recurrent neural network to embed behavioural dynamics."}),"\n",(0,t.jsx)(n.li,{children:"Evaluate the performance of the trained model based on its reconstruction capabilities"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Segment behavior:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Segment pose estimation time series into behavioral motifs, using HMM or K-means."}),"\n",(0,t.jsx)(n.li,{children:"Group similar motifs into communities, using hierarchical clustering."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Analysis:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Optional: Create motif videos to get insights about the fine grained poses."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Create community videos to get more insights about behaviour on a hierarchical scale."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Visualization and projection of latent vectors onto a 2D plane via UMAP."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Use the generative model (reconstruction decoder) to sample from the learned data distribution, reconstruct random real samples or visualize the cluster centre for validation."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Create a video of an egocentrically aligned animal + path through the community space."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["In this tutorial we will show you how to run the VAME workflow using a simple example. The code below can also be found in the ",(0,t.jsx)(n.code,{children:"demo.py"})," script in our Github repository (",(0,t.jsx)(n.a,{href:"https://github.com/EthoML/VAME/blob/main/examples/demo.py",children:"link here"}),")."]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["Check out also the published VAME Workflow Guide, including more hands-on recommendations ",(0,t.jsx)(n.a,{href:"https://www.nature.com/articles/s42003-022-04080-7#Sec8",children:"HERE"}),"."]})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["You can run an entire VAME workflow with just a few lines, using the ",(0,t.jsx)(n.a,{href:"/docs/getting_started/pipeline",children:"Pipeline method"}),"."]})}),"\n",(0,t.jsx)(n.h2,{id:"0-optional-download-input-data",children:"0. [Optional] Download input data"}),"\n",(0,t.jsx)(n.p,{children:"To run VAME you will need a video and a pose estimation file. If you don't have your own data, you download sample data:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from vame.util.sample_data import download_sample_data\n\nsource_software = "DeepLabCut"              # "DeepLabCut", "SLEAP" or "LightningPose"\nps = download_sample_data(source_software)  # Data will be downloaded to ~/.movement/data/\nvideos = [ps["video"]]                      # List of paths to the video files\nposes_estimations = [ps["poses"]]           # List of paths to the pose estimation files\n'})}),"\n",(0,t.jsx)(n.h2,{id:"1-initialize-the-project",children:"1. Initialize the project"}),"\n",(0,t.jsx)(n.p,{children:"VAME organizes around projects. To start a new project, you need to define a few things:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import vame\n\nworking_directory = "."      # The directory where the project will be saved\nproject = "my-vame-project"  # The name of the project\n\n# [Optional] Customized configuration for the project\nconfig_kwargs = {\n    "n_clusters": 15,\n    "pose_confidence": 0.9,\n    "max_epochs": 100,\n}\n\nconfig_path, config = vame.init_new_project(\n    project_name=project_name,\n    videos=videos,\n    poses_estimations=poses_estimations,\n    source_software=source_software,\n    working_directory=working_directory,\n    config_kwargs=config_kwargs,\n)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["This command will create a project folder in the defined working directory with the project name you defined.\nIn this folder you can find a config file called ",(0,t.jsx)(n.a,{href:"/docs/project-config",children:"config.yaml"})," which holds the main parameters for the VAME workflow.\nThe videos and pose estimation files will be linked or copied to the project folder."]}),"\n",(0,t.jsx)(n.h2,{id:"2-preprocess-the-data",children:"2. Preprocess the data"}),"\n",(0,t.jsx)(n.p,{children:"The preprocessing step is responsible for cleaning, filtering and aligning the pose estimation data."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.preprocessing(\n    config=config,\n    centered_reference_keypoint=centered_reference_keypoint,\n    orientation_reference_keypoint=orientation_reference_keypoint,\n)\n"})}),"\n",(0,t.jsx)(n.p,{children:"Internally, this function will:"}),"\n",(0,t.jsx)(n.h3,{id:"21-clean-low-confidence-data-points",children:"2.1 Clean low confidence data points"}),"\n",(0,t.jsx)(n.p,{children:"Pose estimation data points with confidence below the threshold will be cleared and interpolated."}),"\n",(0,t.jsx)(n.h3,{id:"22-egocentric-alignment",children:"2.2 Egocentric alignment"}),"\n",(0,t.jsx)(n.p,{children:"Based on two reference keypoints, the data will be aligned to an egocentric coordinate system:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"centered_reference_keypoint"}),": The keypoint that will be centered in the frame."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"orientation_reference_keypoint"}),": The keypoint that will be used to determine the rotation of the frame."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["By consequence, the ",(0,t.jsx)(n.code,{children:"x"})," and ",(0,t.jsx)(n.code,{children:"y"})," coordinates of the ",(0,t.jsx)(n.code,{children:"centered_reference_keypoint"})," and the ",(0,t.jsx)(n.code,{children:"x"})," coordinate of the ",(0,t.jsx)(n.code,{children:"orientation_reference_keypoint"})," will be set to an array of zeros, and further removed from the dataset."]}),"\n",(0,t.jsx)(n.h3,{id:"23-clean-outliers",children:"2.3 Clean outliers"}),"\n",(0,t.jsxs)(n.p,{children:["Outliers will be removed based on the interquartile range (IQR) method. This means that data points that are below ",(0,t.jsx)(n.code,{children:"Q1 - iqr_factor * IQR"})," or above ",(0,t.jsx)(n.code,{children:"Q3 + iqr_factor * IQR"})," will be cleared and interpolated."]}),"\n",(0,t.jsx)(n.h3,{id:"24-savitzky-golay-filter",children:"2.4 Savitzky-Golay filter"}),"\n",(0,t.jsx)(n.p,{children:"The data will be further smoothed using a Savitzky-Golay filter."}),"\n",(0,t.jsx)(n.h2,{id:"3-train-the-neural-network",children:"3. Train the neural network"}),"\n",(0,t.jsx)(n.p,{children:"At this point, we will prepare the data for training the VAME model, run the training and evaluate the model."}),"\n",(0,t.jsx)(n.h3,{id:"31-create-the-training-dataset",children:"3.1 Create the training dataset"}),"\n",(0,t.jsx)(n.p,{children:"To create the training dataset, which will put the data in the right format for the VAME model, and split it into training and test sets, you can run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.create_trainset(config=config)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"32-training-the-model",children:"3.2 Training the model"}),"\n",(0,t.jsx)(n.p,{children:"Training the vame model might take a while depending on the size of your dataset and your machine settings. To train the model you can run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.train_model(config=config)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"33-evaluate-the-model",children:"3.3 Evaluate the model"}),"\n",(0,t.jsx)(n.p,{children:"THe model evaluation produces two plots, one showing the loss of the model during training and the other showing the reconstruction and future prediction of input sequence."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.evaluate_model(config=config)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"4-segment-behavior",children:"4. Segment behavior"}),"\n",(0,t.jsx)(n.p,{children:"Behavioral segmentation in VAME is done in two steps: pose segmentation into motifs and community detection."}),"\n",(0,t.jsx)(n.h3,{id:"41-pose-segmentation",children:"4.1 Pose segmentation"}),"\n",(0,t.jsx)(n.p,{children:"To perform pose segmentation you can run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.segmentat_session(config=config)\n"})}),"\n",(0,t.jsx)(n.p,{children:"This will perfomr the segmentation using two different algorithms: HMM and K-means. The results will be saved in the project folder."}),"\n",(0,t.jsx)(n.h3,{id:"42-community-detection",children:"4.2 Community detection"}),"\n",(0,t.jsx)(n.p,{children:"Community detection is done by grouping similar motifs into communities using hierarchical clustering. To run community detection you can run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'vame.community(\n    config=config,\n    segmentation_algorithm="hmm",\n    cut_tree=2,\n)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["where ",(0,t.jsx)(n.code,{children:"segmentation_algorithm"}),' can be either "hmm" or "kmeans" and ',(0,t.jsx)(n.code,{children:"cut_tree"})," is the cut level for the hierarchical clustering."]}),"\n",(0,t.jsx)(n.h2,{id:"5-visualize-and-analyze",children:"5. Visualize and analyze"}),"\n",(0,t.jsx)(n.h3,{id:"51-creating-motif-and-community-videos",children:"5.1 Creating motif and community videos"}),"\n",(0,t.jsx)(n.p,{children:"To create motif videos and get insights about the fine grained poses you can run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'vame.motif_videos(\n    config=config,\n    segmentation_algorithm="hmm",\n    video_type=".mp4",\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:"Create community videos:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'vame.community_videos(\n    config=config,\n    segmentation_algorithm="hmm",\n    video_type=".mp4",\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"52-umap-visualization",children:"5.2 UMAP visualization"}),"\n",(0,t.jsx)(n.p,{children:"To visualize and project latent vectors onto a 2D plane via UMAP you can run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from vame.visualization.umap import visualize_umap\n\nvisualize_umap(\n    config=config,\n    label="motif",\n    segmentation_algorithm="hmm",\n)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["where ",(0,t.jsx)(n.code,{children:"label"}),' can be either None, "motif" or "community", and ',(0,t.jsx)(n.code,{children:"segmentation_algorithm"})," can be either 'hmm' or 'kmeans'."]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>s});var t=i(6540);const o={},a=t.createContext(o);function r(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);